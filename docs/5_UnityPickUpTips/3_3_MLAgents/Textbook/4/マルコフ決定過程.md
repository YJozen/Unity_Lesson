マルコフ決定過程（MDP: Markov Decision Process）は、環境の「状態」「行動」「報酬」の関係が完全に分かっている状況下で、エージェントが最適な行動を選択し、報酬を最大化するように進めていくモデルです。  

特徴として、「現在の状態と行動が次の状態を決定する」という「マルコフ性」があります。

<br>

# わかりやすい例：迷路ゲーム
迷路の中でゴールを目指すエージェントを考えます。この迷路の各場所が「状態」を表し、エージェントの行動は「上下左右の移動」です。エージェントはゴールにたどり着いた場合に報酬を得られる仕組みで、以下のようなプロセスで行動が決まります。

1. **状態（State）**:
   - 迷路の各場所を表し、たとえば「スタート地点」「中間地点A」「ゴール地点」などの状態に分かれています。
   
2. **行動（Action）**:
   - エージェントが「上に進む」「下に進む」「右に進む」「左に進む」の4つの方向に移動する行動を取ることができます。
   
3. **遷移（Transition）**:
   - 各行動により次の状態に移ります。たとえば、上に移動すると1つ上のマスに行きますが、壁があれば同じ場所に留まるなど、行動に対して次の状態が確率的に決定されます。

4. **報酬（Reward）**:
   - ゴールに到達すれば高い報酬（例えば+10）を得られ、途中で特定の罠のマスに止まるとマイナスの報酬（例えば-1）を受けます。
   - 他のマスを通過しても報酬はもらえませんが、最終的にゴールを目指して最適な経路を学習します。

<br>

# マルコフ性の特徴
MDPでは、エージェントの次の行動は「現在の状態と行動だけ」に依存し、それ以前の状態や行動の履歴には依存しません。つまり、エージェントは「現在の場所」と「移動方向」だけを基に次の状態に遷移し、累積報酬を最大化することを目指します。

<br>

# MDPの応用例
この考え方は、迷路ゲームだけでなく、以下のような状況でも応用できます：
- **ロボットナビゲーション**：ロボットが部屋のレイアウトに基づき、効率的な移動経路を学習する。
- **チェスやボードゲーム**：現在の盤面（状態）と駒の動かし方（行動）から、勝利（報酬）を目指す。
- **在庫管理**：需要の予測に基づいて在庫の補充（行動）を決定し、コスト削減や利益最大化（報酬）を図る。

マルコフ決定過程を活用すると、このような意思決定の最適化が可能になり、さまざまな環境で効率的な意思決定を支援できます。


<br>

[マルコフ性](マルコフ性.md)

<br>

---

<br>