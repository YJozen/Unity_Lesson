部分観測マルコフ決定過程（POMDP：Partially Observable Markov Decision Process）は、エージェントが環境の全情報を観測できない、つまり観測可能な情報が不完全である状況を扱う強化学習モデルです。  
エージェントは限られた観測情報を基に最適な行動を決定しなければならず、そのためには過去の情報や予測も活用する必要があります。

# 例：霧の中でのロボット探索
霧の中を移動するロボットを考えてみましょう。霧が濃いためにロボットは自分の周囲の情報しか観測できず、全体の位置や目的地との距離がわかりません。  

ロボットの観測には以下の特徴があります：

1. **限られた視野**：  
霧のため、ロボットは半径1メートル以内の障害物や地形しか見えない。

2. **自己位置の不確実性**：  
自分の正確な位置がわからず、方向も時々曖昧になる。

3. **過去の観測と経験が重要**：  
ロボットは過去に通った道や障害物の位置を記憶し、次の行動を選択する際に利用します。


この例では、ロボットが行動を決定するために、現在の観測情報だけでなく、過去の行動や観測から推測した自分の位置や環境情報を元にしなければなりません。たとえば、「前に進む」といった行動が「障害物にぶつからない」「目的地に近づく」といった報酬を得られるようにしますが、観測できる情報が少ないため、報酬を得られる行動を見つけるには、過去の経験を活用する必要があります。

<br>

# POMDPのポイント
- **観測情報が限られている**ため、完全な状態の情報が得られません。
- エージェントは**状態の予測**を行いながら、最適な行動を選択する必要があります。
- **LSTM**などのメモリ構造を使うことで、観測履歴を活かし、現在の最適行動を予測する手助けができます。

このように、POMDPは限られた観測データで最適な決定を行うシナリオで適用されるため、例えばロボットの探索問題や音声認識、金融トレーディングなど、実世界の多くの不確実性のあるシステムに応用されています。

<br>

[マルコフ決定過程](マルコフ決定過程.md)